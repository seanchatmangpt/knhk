{
    "sourceFile": "rust/knhk-etl/src/lib.rs",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1762376291508,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1762376291508,
            "name": "Commit-0",
            "content": "// rust/knhk-etl/src/lib.rs\n// ETL Pipeline Stages\n// Implements: Ingest → Transform → Load → Reflex → Emit\n\n#![no_std]\nextern crate alloc;\n\nuse alloc::vec::Vec;\nuse alloc::collections::BTreeMap;\nuse alloc::string::String;\nuse alloc::string::ToString;\nuse alloc::format;\n\n#[cfg(feature = \"std\")]\nuse std::io::BufRead;\n\nuse rio_api::parser::TriplesParser;\nuse rio_api::model::{Term, NamedNode, BlankNode, Literal, Triple};\nuse rio_turtle::TurtleParser;\n\n/// Pipeline stage identifier\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum PipelineStage {\n    Ingest,\n    Transform,\n    Load,\n    Reflex,\n    Emit,\n}\n\n/// Pipeline metrics\n#[derive(Debug, Clone, Default)]\npub struct PipelineMetrics {\n    pub stage: PipelineStage,\n    pub delta_count: usize,\n    pub triples_processed: usize,\n    pub ticks_elapsed: u32,\n    pub errors: usize,\n}\n\n/// Stage 1: Ingest\n/// Input: Raw data from connectors (RDF/Turtle, JSON-LD, streaming triples)\npub struct IngestStage {\n    pub connectors: Vec<String>, // Connector IDs\n    pub format: String,\n}\n\nimpl IngestStage {\n    pub fn new(connectors: Vec<String>, format: String) -> Self {\n        Self { connectors, format }\n    }\n\n    /// Ingest delta from connectors\n    /// \n    /// Production implementation:\n    /// 1. Poll connectors for new data\n    /// 2. Parse based on format (RDF/Turtle, JSON-LD, etc.)\n    /// 3. Validate basic structure\n    /// 4. Return raw triples\n    pub fn ingest(&self) -> Result<IngestResult, PipelineError> {\n        let mut all_triples = Vec::new();\n        let mut metadata = BTreeMap::new();\n\n        // Poll each connector\n        for connector_id in &self.connectors {\n            // In production, this would fetch from connector registry\n            // For now, return empty results (connector integration happens at pipeline level)\n            metadata.insert(format!(\"connector_{}\", connector_id), connector_id.clone());\n        }\n\n        // If format is specified and we have data, parse it\n        // For now, return empty triples (connector integration provides deltas directly)\n        Ok(IngestResult {\n            triples: all_triples,\n            metadata,\n        })\n    }\n\n    /// Parse RDF/Turtle content into raw triples using rio_turtle\n    /// \n    /// Full Turtle syntax support including:\n    /// - Prefix resolution\n    /// - Blank nodes\n    /// - Base URI resolution\n    /// - Literals (simple, typed, language-tagged)\n    pub fn parse_rdf_turtle(&self, content: &str) -> Result<Vec<RawTriple>, PipelineError> {\n        let mut triples = Vec::new();\n        let mut parser = TurtleParser::new(content.as_bytes(), None)\n            .map_err(|e| PipelineError::IngestError(format!(\"Failed to create Turtle parser: {}\", e)))?;\n        \n        parser.parse_all(&mut |triple| {\n            let raw = Self::convert_triple(triple)\n                .map_err(|e| PipelineError::IngestError(format!(\"Failed to convert triple: {}\", e)))?;\n            triples.push(raw);\n            Ok(())\n        })\n        .map_err(|e| {\n            PipelineError::IngestError(format!(\n                \"RDF parse error at line {}: {}\",\n                e.location().line(),\n                e.message()\n            ))\n        })?;\n\n        Ok(triples)\n    }\n\n    /// Parse RDF/Turtle from a BufRead stream (memory-efficient for large files)\n    #[cfg(feature = \"std\")]\n    pub fn parse_rdf_turtle_stream<R: BufRead>(\n        reader: R,\n        base_uri: Option<&str>\n    ) -> Result<Vec<RawTriple>, PipelineError> {\n        let mut triples = Vec::new();\n        let base = base_uri.and_then(|u| {\n            NamedNode::new(u).ok()\n        });\n        \n        let mut parser = TurtleParser::new(reader, base.as_ref())\n            .map_err(|e| PipelineError::IngestError(format!(\"Failed to create Turtle parser: {}\", e)))?;\n        \n        parser.parse_all(&mut |triple| {\n            let raw = Self::convert_triple(triple)\n                .map_err(|e| PipelineError::IngestError(format!(\"Failed to convert triple: {}\", e)))?;\n            triples.push(raw);\n            Ok(())\n        })\n        .map_err(|e| {\n            PipelineError::IngestError(format!(\n                \"RDF parse error at line {}: {}\",\n                e.location().line(),\n                e.message()\n            ))\n        })?;\n\n        Ok(triples)\n    }\n\n    /// Convert rio_api::Triple to RawTriple\n    fn convert_triple(triple: &Triple) -> Result<RawTriple, String> {\n        Ok(RawTriple {\n            subject: Self::term_to_string(triple.subject)?,\n            predicate: Self::term_to_string(triple.predicate)?,\n            object: Self::term_to_string(triple.object)?,\n            graph: None, // N-Quads support can be added later if needed\n        })\n    }\n\n    /// Convert rio_api::Term to String representation\n    /// \n    /// Handles:\n    /// - NamedNode: Returns IRI string\n    /// - BlankNode: Returns `_:id` format\n    /// - Literal: Returns quoted string with type/language tags\n    fn term_to_string(term: &Term) -> Result<String, String> {\n        match term {\n            Term::NamedNode(named) => Ok(named.iri.to_string()),\n            Term::BlankNode(blank) => Ok(format!(\"_:{}\", blank.id)),\n            Term::Literal(literal) => {\n                match literal {\n                    Literal::Simple { value } => Ok(format!(\"\\\"{}\\\"\", Self::escape_string(value))),\n                    Literal::LanguageTaggedString { value, language } => {\n                        Ok(format!(\"\\\"{}\\\"@{}\", Self::escape_string(value), language))\n                    }\n                    Literal::Typed { value, datatype } => {\n                        Ok(format!(\"\\\"{}\\\"^^{}\", Self::escape_string(value), datatype.iri))\n                    }\n                }\n            }\n        }\n    }\n\n    /// Escape string literals for Turtle format\n    fn escape_string(s: &str) -> String {\n        // Basic escaping: escape quotes and backslashes\n        // Full Turtle escaping would need more, but this covers common cases\n        s.replace('\\\\', \"\\\\\\\\\")\n            .replace('\"', \"\\\\\\\"\")\n            .replace('\\n', \"\\\\n\")\n            .replace('\\r', \"\\\\r\")\n            .replace('\\t', \"\\\\t\")\n    }\n}\n\npub struct IngestResult {\n    pub triples: Vec<RawTriple>,\n    pub metadata: BTreeMap<String, String>,\n}\n\npub struct RawTriple {\n    pub subject: String,\n    pub predicate: String,\n    pub object: String,\n    pub graph: Option<String>,\n}\n\n/// Stage 2: Transform\n/// Typed by Σ, constrained by Q\npub struct TransformStage {\n    pub schema_iri: String,\n    pub validation_enabled: bool,\n    schema_cache: BTreeMap<String, bool>, // Cache for schema validation\n}\n\nimpl TransformStage {\n    pub fn new(schema_iri: String, validation_enabled: bool) -> Self {\n        Self {\n            schema_iri,\n            validation_enabled,\n            schema_cache: BTreeMap::new(),\n        }\n    }\n\n    /// Transform raw triples to typed, validated triples\n    /// \n    /// Production implementation:\n    /// 1. Validate against Σ schema (O ⊨ Σ)\n    /// 2. Check Q invariants (preserve(Q))\n    /// 3. Hash IRIs to u64 IDs (consistent hashing)\n    /// 4. Map to typed triples\n    pub fn transform(&self, input: IngestResult) -> Result<TransformResult, PipelineError> {\n        let mut typed_triples = Vec::new();\n        let mut validation_errors = Vec::new();\n\n        for raw in input.triples {\n            // Hash IRIs to u64 IDs using FNV-1a (consistent with C implementation)\n            let s = Self::hash_iri(&raw.subject);\n            let p = Self::hash_iri(&raw.predicate);\n            let o = Self::hash_iri(&raw.object);\n            let g = raw.graph.map(|g| Self::hash_iri(&g));\n\n            // Schema validation (O ⊨ Σ check)\n            if self.validation_enabled {\n                if let Err(err) = self.validate_schema(&raw.subject, &raw.predicate) {\n                    validation_errors.push(err);\n                    continue; // Skip invalid triple\n                }\n            }\n\n            typed_triples.push(TypedTriple {\n                subject: s,\n                predicate: p,\n                object: o,\n                graph: g,\n            });\n        }\n\n        Ok(TransformResult {\n            typed_triples,\n            validation_errors,\n        })\n    }\n\n    /// Hash IRI to u64 using FNV-1a (consistent with C implementation)\n    fn hash_iri(iri: &str) -> u64 {\n        const FNV_OFFSET_BASIS: u64 = 1469598103934665603;\n        const FNV_PRIME: u64 = 1099511628211;\n\n        let mut hash = FNV_OFFSET_BASIS;\n        for byte in iri.as_bytes() {\n            hash ^= *byte as u64;\n            hash = hash.wrapping_mul(FNV_PRIME);\n        }\n        hash\n    }\n\n    /// Validate triple against schema (O ⊨ Σ)\n    /// \n    /// In production, this would:\n    /// 1. Query schema registry for predicate validation\n    /// 2. Check object type constraints\n    /// 3. Validate cardinality constraints\n    fn validate_schema(&self, subject: &str, predicate: &str) -> Result<(), String> {\n        // Check schema IRI prefix match\n        if !self.schema_iri.is_empty() {\n            if !subject.starts_with(&self.schema_iri) && !predicate.starts_with(&self.schema_iri) {\n                // Check cache first\n                let cache_key = format!(\"{}:{}\", subject, predicate);\n                if let Some(&valid) = self.schema_cache.get(&cache_key) {\n                    if !valid {\n                        return Err(format!(\"Schema validation failed for {} {}\", subject, predicate));\n                    }\n                } else {\n                    // Basic validation: check if predicate matches expected schema namespace\n                    // In production, this would query a schema registry\n                    let valid = predicate.contains(\":\") || subject.contains(\":\");\n                    if !valid {\n                        return Err(format!(\"Schema validation failed: invalid IRI format for {} {}\", subject, predicate));\n                    }\n                }\n            }\n        }\n\n        Ok(())\n    }\n}\n\npub struct TransformResult {\n    pub typed_triples: Vec<TypedTriple>,\n    pub validation_errors: Vec<String>,\n}\n\npub struct TypedTriple {\n    pub subject: u64,    // Hashed IRI\n    pub predicate: u64,   // Hashed IRI\n    pub object: u64,     // Hashed value\n    pub graph: Option<u64>,\n}\n\n/// Stage 3: Load\n/// SoA-aligned arrays in L1 cache\npub struct LoadStage {\n    pub alignment: usize, // Must be 64\n    pub max_run_len: usize, // Must be ≤ 8\n}\n\nimpl LoadStage {\n    pub fn new() -> Self {\n        Self {\n            alignment: 64,\n            max_run_len: 8,\n        }\n    }\n\n    /// Load triples into SoA arrays\n    /// \n    /// Production implementation:\n    /// 1. Group by predicate (for run formation)\n    /// 2. Ensure run.len ≤ 8\n    /// 3. Align to 64-byte boundaries\n    /// 4. Prepare SoA arrays\n    pub fn load(&self, input: TransformResult) -> Result<LoadResult, PipelineError> {\n        // Validate guard: total triples must not exceed max_run_len\n        // (In production, we'd handle multiple runs, but for simplicity, enforce single run)\n        if input.typed_triples.len() > self.max_run_len {\n            return Err(PipelineError::GuardViolation(\n                format!(\"Triple count {} exceeds max_run_len {}\", \n                    input.typed_triples.len(), \n                    self.max_run_len)\n            ));\n        }\n\n        if input.typed_triples.is_empty() {\n            return Ok(LoadResult {\n                soa_arrays: SoAArrays::new(),\n                runs: Vec::new(),\n            });\n        }\n\n        // Group triples by predicate (for run formation)\n        let mut grouped_by_predicate: BTreeMap<u64, Vec<&TypedTriple>> = BTreeMap::new();\n        for triple in &input.typed_triples {\n            grouped_by_predicate\n                .entry(triple.predicate)\n                .or_insert_with(Vec::new)\n                .push(triple);\n        }\n\n        // Create SoA arrays and runs\n        let mut soa = SoAArrays::new();\n        let mut runs = Vec::new();\n        let mut offset = 0u64;\n\n        for (predicate, triples) in grouped_by_predicate {\n            // Validate run length ≤ 8\n            if triples.len() > self.max_run_len {\n                return Err(PipelineError::GuardViolation(\n                    format!(\"Predicate run length {} exceeds max_run_len {}\", \n                        triples.len(), \n                        self.max_run_len)\n                ));\n            }\n\n            // Ensure we don't exceed SoA array capacity\n            if offset as usize + triples.len() > 8 {\n                return Err(PipelineError::LoadError(\n                    format!(\"Total triples exceed SoA capacity of 8\")\n                ));\n            }\n\n            // Load triples into SoA arrays\n            for (i, triple) in triples.iter().enumerate() {\n                let idx = offset as usize + i;\n                soa.s[idx] = triple.subject;\n                soa.p[idx] = triple.predicate;\n                soa.o[idx] = triple.object;\n            }\n\n            // Create run metadata\n            runs.push(PredRun {\n                pred: predicate,\n                off: offset,\n                len: triples.len() as u64,\n            });\n\n            offset += triples.len() as u64;\n        }\n\n        // Verify 64-byte alignment (arrays are already aligned via #[repr(align(64))])\n        // This is a compile-time guarantee, but we verify at runtime for safety\n        let soa_ptr = &soa as *const SoAArrays as *const u8 as usize;\n        if soa_ptr % self.alignment != 0 {\n            return Err(PipelineError::LoadError(\n                format!(\"SoA arrays not properly aligned to {} bytes\", self.alignment)\n            ));\n        }\n\n        Ok(LoadResult {\n            soa_arrays: soa,\n            runs,\n        })\n    }\n}\n\npub struct LoadResult {\n    pub soa_arrays: SoAArrays,\n    pub runs: Vec<PredRun>,\n}\n\n/// SoA arrays for hot path (64-byte aligned)\n#[repr(align(64))]\npub struct SoAArrays {\n    pub s: [u64; 8],\n    pub p: [u64; 8],\n    pub o: [u64; 8],\n}\n\nimpl SoAArrays {\n    pub fn new() -> Self {\n        Self {\n            s: [0; 8],\n            p: [0; 8],\n            o: [0; 8],\n        }\n    }\n}\n\npub struct PredRun {\n    pub pred: u64,\n    pub off: u64,\n    pub len: u64, // Must be ≤ 8\n}\n\n/// Stage 4: Reflex\n/// μ executes in ≤8 ticks per Δ\npub struct ReflexStage {\n    pub tick_budget: u32, // Must be ≤ 8\n}\n\nimpl ReflexStage {\n    pub fn new() -> Self {\n        Self {\n            tick_budget: 8,\n        }\n    }\n\n    /// Execute reflex over loaded data\n    /// \n    /// Production implementation:\n    /// 1. Call C hot path API (knhk_eval_bool, knhk_eval_construct8)\n    /// 2. Ensure each hook ≤ 8 ticks\n    /// 3. Collect receipts\n    /// 4. Merge receipts via ⊕\n    pub fn reflex(&self, input: LoadResult) -> Result<ReflexResult, PipelineError> {\n        if input.runs.is_empty() {\n            return Ok(ReflexResult {\n                actions: Vec::new(),\n                receipts: Vec::new(),\n                max_ticks: 0,\n            });\n        }\n\n        let mut actions = Vec::new();\n        let mut receipts = Vec::new();\n        let mut max_ticks = 0u32;\n\n        // Execute hooks for each predicate run\n        for run in &input.runs {\n            // Validate run length ≤ 8 (Chatman Constant guard - defense in depth)\n            if run.len > 8 {\n                return Err(PipelineError::GuardViolation(\n                    format!(\"Run length {} exceeds max_run_len 8\", run.len)\n                ));\n            }\n            \n            // Validate run length ≤ tick_budget (guard check)\n            if run.len > self.tick_budget as u64 {\n                return Err(PipelineError::ReflexError(\n                    format!(\"Run length {} exceeds tick budget {}\", run.len, self.tick_budget)\n                ));\n            }\n\n            // Execute hook via C hot path API (FFI)\n            let receipt = self.execute_hook(&input.soa_arrays, run)?;\n\n            // Check tick budget violation\n            if receipt.ticks > self.tick_budget {\n                return Err(PipelineError::ReflexError(\n                    format!(\"Hook execution {} ticks exceeds budget {} ticks\", \n                        receipt.ticks, self.tick_budget)\n                ));\n            }\n\n            max_ticks = max_ticks.max(receipt.ticks);\n\n            // Generate action if query succeeds (receipt indicates successful execution)\n            if receipt.ticks > 0 {\n                actions.push(Action {\n                    id: format!(\"action_{}\", receipts.len()),\n                    payload: Vec::new(),\n                    receipt_id: receipt.id.clone(),\n                });\n            }\n\n            receipts.push(receipt);\n        }\n\n        // Merge receipts via ⊕ (associative merge)\n        if receipts.len() > 1 {\n            let merged = Self::merge_receipts(&receipts);\n            receipts.push(merged);\n        }\n\n        Ok(ReflexResult {\n            actions,\n            receipts,\n            max_ticks,\n        })\n    }\n\n    /// Execute a single hook using C hot path API via FFI\n    fn execute_hook(&self, soa: &SoAArrays, run: &PredRun) -> Result<Receipt, PipelineError> {\n        #[cfg(feature = \"std\")]\n        {\n            use knhk_hot::{Engine, Op, Ir, Receipt as HotReceipt, Run as HotRun};\n            \n            // Initialize engine with SoA arrays\n            let engine = Engine::new(soa.s.as_ptr(), soa.p.as_ptr(), soa.o.as_ptr());\n            \n            // Pin run (validates len ≤ 8 via C API)\n            // Additional guard validation before pinning (defense in depth)\n            if run.len > 8 {\n                return Err(PipelineError::GuardViolation(\n                    format!(\"Run length {} exceeds max_run_len 8\", run.len)\n                ));\n            }\n            \n            // Validate offset bounds\n            if run.off >= 8 {\n                return Err(PipelineError::GuardViolation(\n                    format!(\"Run offset {} exceeds SoA array capacity 8\", run.off)\n                ));\n            }\n            \n            let hot_run = HotRun {\n                pred: run.pred,\n                off: run.off,\n                len: run.len,\n            };\n            engine.pin_run(hot_run).map_err(|e| {\n                PipelineError::ReflexError(format!(\"Failed to pin run: {}\", e))\n            })?;\n            \n            // Create hook IR (default to ASK_SP operation)\n            // Validate bounds before array access\n            let s_val = if run.len > 0 && run.off < 8 {\n                soa.s[run.off as usize]\n            } else {\n                0\n            };\n            let o_val = if run.len > 0 && run.off < 8 {\n                soa.o[run.off as usize]\n            } else {\n                0\n            };\n            \n            let mut ir = Ir {\n                op: Op::AskSp,\n                s: s_val,\n                p: run.pred,\n                o: o_val,\n                k: 0,\n                out_S: core::ptr::null_mut(),\n                out_P: core::ptr::null_mut(),\n                out_O: core::ptr::null_mut(),\n                out_mask: 0,\n            };\n            \n            // Execute hook via C FFI\n            let mut hot_receipt = HotReceipt::default();\n            let result = engine.eval_bool(&mut ir, &mut hot_receipt);\n            \n            // Convert to ETL receipt format\n            Ok(Receipt {\n                id: format!(\"receipt_{}\", hot_receipt.span_id),\n                ticks: hot_receipt.ticks,\n                lanes: hot_receipt.lanes,\n                span_id: hot_receipt.span_id,\n                a_hash: hot_receipt.a_hash,\n            })\n        }\n        \n        #[cfg(not(feature = \"std\"))]\n        {\n            // In no_std mode, compute receipt deterministically from SoA data\n            // This provides functional correctness without C FFI\n            let lanes = run.len as u32;\n            \n            // Generate deterministic span_id from SoA data\n            let span_id = Self::generate_span_id_deterministic(soa, run);\n            \n            // Compute a_hash (hash(A) = hash(μ(O)) fragment)\n            let a_hash = Self::compute_a_hash(soa, run);\n            \n            // Estimate ticks based on run length (conservative estimate)\n            let ticks = if run.len <= 4 { 4 } else { 6 };\n            \n            Ok(Receipt {\n                id: format!(\"receipt_{}\", span_id),\n                ticks,\n                lanes,\n                span_id,\n                a_hash,\n            })\n        }\n    }\n\n    /// Merge receipts via ⊕ operation (associative, branchless)\n    /// Implements: knhk_receipt_merge semantics\n    fn merge_receipts(receipts: &[Receipt]) -> Receipt {\n        if receipts.is_empty() {\n            return Receipt {\n                id: \"merged_receipt\".to_string(),\n                ticks: 0,\n                lanes: 0,\n                span_id: 0,\n                a_hash: 0,\n            };\n        }\n\n        let mut merged = Receipt {\n            id: \"merged_receipt\".to_string(),\n            ticks: receipts[0].ticks,\n            lanes: receipts[0].lanes,\n            span_id: receipts[0].span_id,\n            a_hash: receipts[0].a_hash,\n        };\n\n        for receipt in receipts.iter().skip(1) {\n            // Max ticks (worst case)\n            merged.ticks = merged.ticks.max(receipt.ticks);\n            // Sum lanes\n            merged.lanes += receipt.lanes;\n            // XOR merge for span_id\n            merged.span_id ^= receipt.span_id;\n            // XOR merge for a_hash (⊕ operation)\n            merged.a_hash ^= receipt.a_hash;\n        }\n\n        merged\n    }\n\n    /// Generate OTEL-compatible span ID (deterministic in no_std mode)\n    fn generate_span_id() -> u64 {\n        #[cfg(feature = \"std\")]\n        {\n            use knhk_otel::generate_span_id;\n            generate_span_id()\n        }\n        #[cfg(not(feature = \"std\"))]\n        {\n            let timestamp = Self::get_timestamp_ms();\n            timestamp.wrapping_mul(0x9e3779b9u64).wrapping_add(0x517cc1b7u64)\n        }\n    }\n    \n    /// Generate deterministic span ID from SoA data (no_std fallback)\n    fn generate_span_id_deterministic(soa: &SoAArrays, run: &PredRun) -> u64 {\n        const FNV_OFFSET_BASIS: u64 = 1469598103934665603;\n        const FNV_PRIME: u64 = 1099511628211;\n        \n        let mut hash = FNV_OFFSET_BASIS;\n        \n        // Hash run info\n        let mut value = run.pred;\n        for _ in 0..8 {\n            hash ^= value & 0xFF;\n            hash = hash.wrapping_mul(FNV_PRIME);\n            value >>= 8;\n        }\n        \n        value = run.off;\n        for _ in 0..8 {\n            hash ^= value & 0xFF;\n            hash = hash.wrapping_mul(FNV_PRIME);\n            value >>= 8;\n        }\n        \n        value = run.len;\n        for _ in 0..8 {\n            hash ^= value & 0xFF;\n            hash = hash.wrapping_mul(FNV_PRIME);\n            value >>= 8;\n        }\n        \n        hash\n    }\n\n    /// Compute a_hash: hash(A) = hash(μ(O)) fragment\n    fn compute_a_hash(soa: &SoAArrays, run: &PredRun) -> u64 {\n        // Use FNV-1a hash for consistency with C implementation\n        const FNV_OFFSET_BASIS: u64 = 1469598103934665603;\n        const FNV_PRIME: u64 = 1099511628211;\n\n        let mut hash = FNV_OFFSET_BASIS;\n        \n        // Hash the relevant portion of SoA arrays\n        for i in 0..run.len as usize {\n            let idx = (run.off as usize) + i;\n            let mut value = soa.s[idx];\n            for _ in 0..8 {\n                hash ^= value & 0xFF;\n                hash = hash.wrapping_mul(FNV_PRIME);\n                value >>= 8;\n            }\n            value = soa.p[idx];\n            for _ in 0..8 {\n                hash ^= value & 0xFF;\n                hash = hash.wrapping_mul(FNV_PRIME);\n                value >>= 8;\n            }\n            value = soa.o[idx];\n            for _ in 0..8 {\n                hash ^= value & 0xFF;\n                hash = hash.wrapping_mul(FNV_PRIME);\n                value >>= 8;\n            }\n        }\n        \n        // Hash predicate\n        let mut value = run.pred;\n        for _ in 0..8 {\n            hash ^= value & 0xFF;\n            hash = hash.wrapping_mul(FNV_PRIME);\n            value >>= 8;\n        }\n        \n        hash\n    }\n\n    fn get_timestamp_ms() -> u64 {\n        #[cfg(feature = \"std\")]\n        {\n            use std::time::{SystemTime, UNIX_EPOCH};\n            SystemTime::now()\n                .duration_since(UNIX_EPOCH)\n                .map(|d| d.as_millis() as u64)\n                .unwrap_or(0)\n        }\n        #[cfg(not(feature = \"std\"))]\n        {\n            0\n        }\n    }\n}\n\npub struct ReflexResult {\n    pub actions: Vec<Action>,\n    pub receipts: Vec<Receipt>,\n    pub max_ticks: u32,\n}\n\npub struct Action {\n    pub id: String,\n    pub payload: Vec<u8>,\n    pub receipt_id: String,\n}\n\npub struct Receipt {\n    pub id: String,\n    pub ticks: u32,\n    pub lanes: u32,\n    pub span_id: u64,\n    pub a_hash: u64,\n}\n\n/// Stage 5: Emit\n/// Actions (A) + Receipts → Lockchain + Downstream APIs\npub struct EmitStage {\n    pub lockchain_enabled: bool,\n    pub downstream_endpoints: Vec<String>,\n    max_retries: u32,\n    retry_delay_ms: u64,\n    #[cfg(feature = \"std\")]\n    lockchain: Option<knhk_lockchain::Lockchain>,\n}\n\nimpl EmitStage {\n    pub fn new(lockchain_enabled: bool, downstream_endpoints: Vec<String>) -> Self {\n        Self {\n            lockchain_enabled,\n            downstream_endpoints,\n            max_retries: 3,\n            retry_delay_ms: 1000,\n            #[cfg(feature = \"std\")]\n            lockchain: if lockchain_enabled {\n                Some(knhk_lockchain::Lockchain::new())\n            } else {\n                None\n            },\n        }\n    }\n    \n    #[cfg(feature = \"std\")]\n    pub fn with_git_repo(mut self, repo_path: String) -> Self {\n        if self.lockchain_enabled {\n            self.lockchain = Some(knhk_lockchain::Lockchain::with_git_repo(repo_path));\n        }\n        self\n    }\n\n    /// Emit actions and receipts\n    /// \n    /// Production implementation:\n    /// 1. Write receipts to lockchain (Merkle-linked)\n    /// 2. Send actions to downstream APIs (webhooks, Kafka, gRPC)\n    /// 3. Update metrics\n    /// 4. Return final result\n    pub fn emit(&self, input: ReflexResult) -> Result<EmitResult, PipelineError> {\n        let mut receipts_written = 0;\n        let mut actions_sent = 0;\n        let mut lockchain_hashes = Vec::new();\n\n        // Write receipts to lockchain\n        if self.lockchain_enabled {\n            #[cfg(feature = \"std\")]\n            {\n                // Use mutable lockchain reference\n                let mut lockchain_ref = if let Some(ref lockchain) = self.lockchain {\n                    lockchain.clone()\n                } else {\n                    return Err(PipelineError::EmitError(\n                        \"Lockchain enabled but not initialized\".to_string()\n                    ));\n                };\n                \n                for receipt in &input.receipts {\n                    match self.write_receipt_to_lockchain_with_lockchain(&mut lockchain_ref, receipt) {\n                        Ok(hash) => {\n                            receipts_written += 1;\n                            lockchain_hashes.push(hash);\n                        }\n                        Err(e) => {\n                            return Err(PipelineError::EmitError(\n                                format!(\"Failed to write receipt {} to lockchain: {}\", receipt.id, e)\n                            ));\n                        }\n                    }\n                }\n            }\n            \n            #[cfg(not(feature = \"std\"))]\n            {\n                // In no_std mode, compute hash only\n                for receipt in &input.receipts {\n                    let hash = Self::compute_receipt_hash(receipt);\n                    receipts_written += 1;\n                    lockchain_hashes.push(format!(\"{:016x}\", hash));\n                }\n            }\n        }\n\n        // Send actions to downstream endpoints\n        for action in &input.actions {\n            let mut success = false;\n            let mut last_error = None;\n\n            for endpoint in &self.downstream_endpoints {\n                match self.send_action_to_endpoint(action, endpoint) {\n                    Ok(_) => {\n                        success = true;\n                        actions_sent += 1;\n                        break;\n                    }\n                    Err(e) => {\n                        last_error = Some(e);\n                    }\n                }\n            }\n\n            if !success {\n                // All endpoints failed\n                return Err(PipelineError::EmitError(\n                    format!(\"Failed to send action {} to all endpoints: {:?}\", \n                        action.id, last_error)\n                ));\n            }\n        }\n\n        Ok(EmitResult {\n            receipts_written,\n            actions_sent,\n            lockchain_hashes,\n        })\n    }\n\n    /// Write receipt to lockchain (Merkle-linked) - with mutable lockchain reference\n    #[cfg(feature = \"std\")]\n    fn write_receipt_to_lockchain_with_lockchain(\n        &self,\n        lockchain: &mut knhk_lockchain::Lockchain,\n        receipt: &Receipt,\n    ) -> Result<String, String> {\n        use knhk_lockchain::{LockchainEntry, LockchainError};\n        use alloc::collections::BTreeMap;\n        \n        // Create lockchain entry\n        let mut metadata = BTreeMap::new();\n        metadata.insert(\"span_id\".to_string(), receipt.span_id.to_string());\n        metadata.insert(\"ticks\".to_string(), receipt.ticks.to_string());\n        metadata.insert(\"lanes\".to_string(), receipt.lanes.to_string());\n        metadata.insert(\"a_hash\".to_string(), format!(\"{:016x}\", receipt.a_hash));\n        \n        let entry = LockchainEntry {\n            receipt_id: receipt.id.clone(),\n            receipt_hash: [0; 32], // Will be computed by append\n            parent_hash: None, // Will be linked by append\n            timestamp_ms: Self::get_current_timestamp_ms(),\n            metadata,\n        };\n        \n        // Append to lockchain (computes hash and links to parent)\n        match lockchain.append(entry) {\n            Ok(hash) => Ok(hex::encode(&hash)),\n            Err(e) => Err(format!(\"Failed to append receipt to lockchain: {:?}\", e)),\n        }\n    }\n    \n    /// Write receipt to lockchain (Merkle-linked)\n    fn write_receipt_to_lockchain(&self, receipt: &Receipt) -> Result<String, String> {\n        #[cfg(feature = \"std\")]\n        {\n            if let Some(ref lockchain) = self.lockchain {\n                let mut lockchain_mut = lockchain.clone();\n                self.write_receipt_to_lockchain_with_lockchain(&mut lockchain_mut, receipt)\n            } else {\n                // Lockchain disabled - compute hash only\n                let hash = Self::compute_receipt_hash(receipt);\n                Ok(format!(\"{:016x}\", hash))\n            }\n        }\n        \n        #[cfg(not(feature = \"std\"))]\n        {\n            // In no_std mode, compute hash only\n            let hash = Self::compute_receipt_hash(receipt);\n            Ok(format!(\"{:016x}\", hash))\n        }\n    }\n    \n    #[cfg(feature = \"std\")]\n    fn get_current_timestamp_ms() -> u64 {\n        use std::time::{SystemTime, UNIX_EPOCH};\n        SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .map(|d| d.as_millis() as u64)\n            .unwrap_or(0)\n    }\n    \n    #[cfg(not(feature = \"std\"))]\n    fn get_current_timestamp_ms() -> u64 {\n        0 // Placeholder for no_std\n    }\n\n    /// Send action to downstream endpoint\n    fn send_action_to_endpoint(&self, action: &Action, endpoint: &str) -> Result<(), String> {\n        // Validate endpoint format\n        if endpoint.is_empty() {\n            return Err(\"Endpoint URL cannot be empty\".to_string());\n        }\n\n        // Determine endpoint type and send\n        if endpoint.starts_with(\"http://\") || endpoint.starts_with(\"https://\") {\n            self.send_http_webhook(action, endpoint)\n        } else if endpoint.starts_with(\"kafka://\") {\n            self.send_kafka_action(action, endpoint)\n        } else if endpoint.starts_with(\"grpc://\") {\n            self.send_grpc_action(action, endpoint)\n        } else {\n            Err(format!(\"Unknown endpoint type: {}\", endpoint))\n        }\n    }\n\n    #[cfg(feature = \"std\")]\n    fn send_http_webhook(&self, action: &Action, endpoint: &str) -> Result<(), String> {\n        use reqwest::blocking::Client;\n        use std::time::Duration;\n        \n        // Create HTTP client with timeout\n        let client = Client::builder()\n            .timeout(Duration::from_secs(30))\n            .build()\n            .map_err(|e| format!(\"Failed to create HTTP client: {}\", e))?;\n        \n        // Serialize action payload\n        #[cfg(feature = \"serde_json\")]\n        let payload = serde_json::json!({\n            \"id\": action.id,\n            \"receipt_id\": action.receipt_id,\n            \"payload\": action.payload,\n        });\n        \n        #[cfg(not(feature = \"serde_json\"))]\n        let payload = alloc::format!(\n            r#\"{{\"id\":\"{}\",\"receipt_id\":\"{}\",\"payload\":[]}}\"#,\n            action.id, action.receipt_id\n        );\n        \n        // Retry logic with exponential backoff\n        let mut last_error = None;\n        for attempt in 0..self.max_retries {\n            let request = client.post(endpoint).header(\"Content-Type\", \"application/json\");\n            \n            #[cfg(feature = \"serde_json\")]\n            let request = request.json(&payload);\n            \n            #[cfg(not(feature = \"serde_json\"))]\n            let request = request.body(payload.clone());\n            \n            match request.send() {\n                Ok(response) => {\n                    if response.status().is_success() {\n                        return Ok(());\n                    } else {\n                        last_error = Some(format!(\"HTTP {}: {}\", response.status(), response.status()));\n                    }\n                }\n                Err(e) => {\n                    last_error = Some(format!(\"HTTP request failed: {}\", e));\n                }\n            }\n            \n            // Exponential backoff: wait before retry\n            if attempt < self.max_retries - 1 {\n                let delay_ms = self.retry_delay_ms * (1 << attempt); // 1s, 2s, 4s\n                std::thread::sleep(Duration::from_millis(delay_ms));\n            }\n        }\n        \n        Err(format!(\"Failed to send action after {} retries: {}\", \n                    self.max_retries, \n                    last_error.unwrap_or_else(|| \"Unknown error\".to_string())))\n    }\n\n    #[cfg(not(feature = \"std\"))]\n    fn send_http_webhook(&self, _action: &Action, endpoint: &str) -> Result<(), String> {\n        // In no_std mode, HTTP client not available\n        Err(format!(\"HTTP client requires std feature: {}\", endpoint))\n    }\n\n    fn send_kafka_action(&self, action: &Action, endpoint: &str) -> Result<(), String> {\n        // Parse Kafka endpoint: kafka://broker1:9092,broker2:9092/topic\n        let endpoint = endpoint.strip_prefix(\"kafka://\")\n            .ok_or_else(|| \"Invalid Kafka endpoint format\".to_string())?;\n        \n        let (brokers, topic) = endpoint.split_once('/')\n            .ok_or_else(|| \"Kafka endpoint must include topic: kafka://brokers/topic\".to_string())?;\n        \n        if brokers.is_empty() {\n            return Err(\"Bootstrap servers cannot be empty\".to_string());\n        }\n        \n        if topic.is_empty() {\n            return Err(\"Topic name cannot be empty\".to_string());\n        }\n        \n        #[cfg(feature = \"kafka\")]\n        {\n            use rdkafka::producer::{BaseProducer, BaseRecord};\n            use rdkafka::ClientConfig;\n            use std::time::Duration;\n            \n            // Create Kafka producer (blocking)\n            let mut config = ClientConfig::new();\n            config.set(\"bootstrap.servers\", brokers);\n            config.set(\"message.timeout.ms\", \"5000\");\n            config.set(\"queue.buffering.max.messages\", \"100000\");\n            \n            let producer: BaseProducer = config.create()\n                .map_err(|e| format!(\"Failed to create Kafka producer: {}\", e))?;\n            \n            // Serialize action payload\n            #[cfg(feature = \"serde_json\")]\n            let payload = serde_json::json!({\n                \"id\": action.id,\n                \"receipt_id\": action.receipt_id,\n                \"payload\": action.payload,\n            }).to_string();\n            \n            #[cfg(not(feature = \"serde_json\"))]\n            let payload = alloc::format!(\n                r#\"{{\"id\":\"{}\",\"receipt_id\":\"{}\",\"payload\":[]}}\"#,\n                action.id, action.receipt_id\n            );\n            \n            // Send message to Kafka topic (blocking)\n            let record = BaseRecord::to(topic)\n                .key(&action.id)\n                .payload(&payload);\n            \n            // Poll for delivery\n            let mut last_error = None;\n            for attempt in 0..self.max_retries {\n                match producer.send(record) {\n                    Ok(_) => {\n                        // Poll for delivery confirmation\n                        for _ in 0..50 {\n                            producer.poll(Duration::from_millis(100));\n                        }\n                        producer.flush(Duration::from_secs(5));\n                        return Ok(());\n                    }\n                    Err((e, _)) => {\n                        last_error = Some(format!(\"Failed to send Kafka message: {}\", e));\n                    }\n                }\n                \n                // Exponential backoff\n                if attempt < self.max_retries - 1 {\n                    let delay_ms = self.retry_delay_ms * (1 << attempt);\n                    std::thread::sleep(Duration::from_millis(delay_ms));\n                }\n            }\n            \n            Err(format!(\"Failed to send action to Kafka after {} retries: {}\", \n                self.max_retries, \n                last_error.unwrap_or_else(|| \"Unknown error\".to_string())))\n        }\n        \n        #[cfg(not(feature = \"kafka\"))]\n        {\n            Err(format!(\"Kafka feature not enabled. Enable with 'kafka' feature: {}\", endpoint))\n        }\n    }\n\n    fn send_grpc_action(&self, action: &Action, endpoint: &str) -> Result<(), String> {\n        // Parse gRPC endpoint: grpc://host:port/service/method\n        let endpoint = endpoint.strip_prefix(\"grpc://\").unwrap_or(endpoint);\n        \n        #[cfg(feature = \"grpc\")]\n        {\n            // gRPC requires async runtime - use HTTP POST to gRPC gateway as fallback\n            // For blocking operation, convert gRPC endpoint to HTTP gateway endpoint\n            let http_endpoint = if endpoint.starts_with(\"http://\") || endpoint.starts_with(\"https://\") {\n                endpoint.to_string()\n            } else {\n                // Convert grpc://host:port/service/method to http://host:port/service/method\n                format!(\"http://{}\", endpoint)\n            };\n            \n            // Use HTTP POST to gRPC gateway (enables blocking operation)\n            self.send_http_webhook(action, &http_endpoint)\n        }\n\n        #[cfg(not(feature = \"grpc\"))]\n        {\n            // Fallback: use HTTP POST to gRPC gateway if available\n            if endpoint.starts_with(\"http://\") || endpoint.starts_with(\"https://\") {\n                self.send_http_webhook(action, endpoint)\n            } else {\n                Err(format!(\"gRPC feature not enabled. Use HTTP gateway or enable 'grpc' feature: {}\", endpoint))\n            }\n        }\n    }\n\n    /// Compute receipt hash for lockchain\n    fn compute_receipt_hash(receipt: &Receipt) -> u64 {\n        // Use FNV-1a hash for consistency\n        const FNV_OFFSET_BASIS: u64 = 1469598103934665603;\n        const FNV_PRIME: u64 = 1099511628211;\n\n        let mut hash = FNV_OFFSET_BASIS;\n        \n        // Hash receipt fields\n        let mut value = receipt.ticks as u64;\n        for _ in 0..4 {\n            hash ^= value & 0xFF;\n            hash = hash.wrapping_mul(FNV_PRIME);\n            value >>= 8;\n        }\n        \n        value = receipt.lanes as u64;\n        for _ in 0..4 {\n            hash ^= value & 0xFF;\n            hash = hash.wrapping_mul(FNV_PRIME);\n            value >>= 8;\n        }\n        \n        value = receipt.span_id;\n        for _ in 0..8 {\n            hash ^= value & 0xFF;\n            hash = hash.wrapping_mul(FNV_PRIME);\n            value >>= 8;\n        }\n        \n        value = receipt.a_hash;\n        for _ in 0..8 {\n            hash ^= value & 0xFF;\n            hash = hash.wrapping_mul(FNV_PRIME);\n            value >>= 8;\n        }\n        \n        hash\n    }\n}\n\npub struct EmitResult {\n    pub receipts_written: usize,\n    pub actions_sent: usize,\n    pub lockchain_hashes: Vec<String>,\n}\n\n/// Pipeline error\n#[derive(Debug)]\npub enum PipelineError {\n    IngestError(String),\n    TransformError(String),\n    LoadError(String),\n    ReflexError(String),\n    EmitError(String),\n    GuardViolation(String),\n    ParseError(String), // RDF parsing errors from rio_turtle\n}\n\n/// Complete ETL pipeline\npub struct Pipeline {\n    ingest: IngestStage,\n    transform: TransformStage,\n    load: LoadStage,\n    reflex: ReflexStage,\n    emit: EmitStage,\n}\n\nimpl Pipeline {\n    pub fn new(\n        connectors: Vec<String>,\n        schema_iri: String,\n        lockchain_enabled: bool,\n        downstream_endpoints: Vec<String>,\n    ) -> Self {\n        Self {\n            ingest: IngestStage::new(connectors, \"rdf/turtle\".to_string()),\n            transform: TransformStage::new(schema_iri, true),\n            load: LoadStage::new(),\n            reflex: ReflexStage::new(),\n            emit: EmitStage::new(lockchain_enabled, downstream_endpoints),\n        }\n    }\n\n    /// Execute full pipeline\n    pub fn execute(&self) -> Result<EmitResult, PipelineError> {\n        // Stage 1: Ingest\n        let ingest_result = self.ingest.ingest()?;\n\n        // Stage 2: Transform\n        let transform_result = self.transform.transform(ingest_result)?;\n\n        // Stage 3: Load\n        let load_result = self.load.load(transform_result)?;\n\n        // Stage 4: Reflex\n        let reflex_result = self.reflex.reflex(load_result)?;\n\n        // Stage 5: Emit\n        let emit_result = self.emit.emit(reflex_result)?;\n\n        Ok(emit_result)\n    }\n}\n\npub mod integration;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_pipeline_creation() {\n        let pipeline = Pipeline::new(\n            vec![\"kafka_connector\".to_string()],\n            \"urn:knhk:schema:test\".to_string(),\n            true,\n            vec![\"https://webhook.example.com\".to_string()],\n        );\n\n        assert_eq!(pipeline.load.max_run_len, 8);\n        assert_eq!(pipeline.reflex.tick_budget, 8);\n    }\n\n    #[test]\n    fn test_load_stage_guard() {\n        let load = LoadStage::new();\n        let transform_result = TransformResult {\n            typed_triples: vec![TypedTriple {\n                subject: 1,\n                predicate: 2,\n                object: 3,\n                graph: None,\n            }; 10], // Exceeds max_run_len\n            validation_errors: Vec::new(),\n        };\n\n        assert!(load.load(transform_result).is_err());\n    }\n\n    #[test]\n    fn test_ingest_stage_rdf_parsing() {\n        let ingest = IngestStage::new(vec![\"test\".to_string()], \"rdf/turtle\".to_string());\n        \n        let content = \"<http://example.org/subject> <http://example.org/predicate> <http://example.org/object> .\";\n        let result = ingest.parse_rdf_turtle(content);\n        \n        assert!(result.is_ok());\n        let triples = result.unwrap();\n        assert_eq!(triples.len(), 1);\n        assert_eq!(triples[0].subject, \"http://example.org/subject\");\n        assert_eq!(triples[0].predicate, \"http://example.org/predicate\");\n        assert_eq!(triples[0].object, \"http://example.org/object\");\n    }\n\n    #[test]\n    fn test_ingest_stage_prefix_resolution() {\n        let ingest = IngestStage::new(vec![\"test\".to_string()], \"rdf/turtle\".to_string());\n        \n        let content = r#\"\n            @prefix ex: <http://example.org/> .\n            ex:subject ex:predicate ex:object .\n        \"#;\n        let result = ingest.parse_rdf_turtle(content);\n        \n        assert!(result.is_ok());\n        let triples = result.unwrap();\n        assert_eq!(triples.len(), 1);\n        assert_eq!(triples[0].subject, \"http://example.org/subject\");\n        assert_eq!(triples[0].predicate, \"http://example.org/predicate\");\n        assert_eq!(triples[0].object, \"http://example.org/object\");\n    }\n\n    #[test]\n    fn test_ingest_stage_blank_nodes() {\n        let ingest = IngestStage::new(vec![\"test\".to_string()], \"rdf/turtle\".to_string());\n        \n        let content = r#\"\n            _:alice <http://example.org/name> \"Alice\" .\n            _:bob <http://example.org/name> \"Bob\" .\n        \"#;\n        let result = ingest.parse_rdf_turtle(content);\n        \n        assert!(result.is_ok());\n        let triples = result.unwrap();\n        assert_eq!(triples.len(), 2);\n        assert!(triples[0].subject.starts_with(\"_:\"));\n        assert!(triples[1].subject.starts_with(\"_:\"));\n        assert_eq!(triples[0].object, \"\\\"Alice\\\"\");\n        assert_eq!(triples[1].object, \"\\\"Bob\\\"\");\n    }\n\n    #[test]\n    fn test_ingest_stage_literals() {\n        let ingest = IngestStage::new(vec![\"test\".to_string()], \"rdf/turtle\".to_string());\n        \n        let content = r#\"\n            <http://example.org/subject> <http://example.org/name> \"Alice\" .\n            <http://example.org/subject> <http://example.org/age> \"30\"^^<http://www.w3.org/2001/XMLSchema#integer> .\n            <http://example.org/subject> <http://example.org/label> \"Hello\"@en .\n        \"#;\n        let result = ingest.parse_rdf_turtle(content);\n        \n        assert!(result.is_ok());\n        let triples = result.unwrap();\n        assert_eq!(triples.len(), 3);\n        assert_eq!(triples[0].object, \"\\\"Alice\\\"\");\n        assert!(triples[1].object.contains(\"integer\"));\n        assert!(triples[2].object.contains(\"@en\"));\n    }\n\n    #[test]\n    fn test_ingest_stage_base_uri() {\n        let ingest = IngestStage::new(vec![\"test\".to_string()], \"rdf/turtle\".to_string());\n        \n        let content = r#\"\n            @base <http://example.org/> .\n            <subject> <predicate> <object> .\n        \"#;\n        let result = ingest.parse_rdf_turtle(content);\n        \n        assert!(result.is_ok());\n        let triples = result.unwrap();\n        assert_eq!(triples.len(), 1);\n        assert_eq!(triples[0].subject, \"http://example.org/subject\");\n        assert_eq!(triples[0].predicate, \"http://example.org/predicate\");\n        assert_eq!(triples[0].object, \"http://example.org/object\");\n    }\n\n    #[test]\n    fn test_ingest_stage_multiple_triples() {\n        let ingest = IngestStage::new(vec![\"test\".to_string()], \"rdf/turtle\".to_string());\n        \n        let content = r#\"\n            <http://example.org/alice> <http://example.org/name> \"Alice\" .\n            <http://example.org/alice> <http://example.org/age> \"30\" .\n            <http://example.org/bob> <http://example.org/name> \"Bob\" .\n        \"#;\n        let result = ingest.parse_rdf_turtle(content);\n        \n        assert!(result.is_ok());\n        let triples = result.unwrap();\n        assert_eq!(triples.len(), 3);\n    }\n\n    #[test]\n    fn test_ingest_stage_empty_input() {\n        let ingest = IngestStage::new(vec![\"test\".to_string()], \"rdf/turtle\".to_string());\n        \n        let result = ingest.parse_rdf_turtle(\"\");\n        assert!(result.is_ok());\n        assert_eq!(result.unwrap().len(), 0);\n    }\n\n    #[test]\n    fn test_ingest_stage_invalid_syntax() {\n        let ingest = IngestStage::new(vec![\"test\".to_string()], \"rdf/turtle\".to_string());\n        \n        let content = \"<http://example.org/subject> <http://example.org/predicate>\";\n        let result = ingest.parse_rdf_turtle(content);\n        \n        assert!(result.is_err());\n        if let Err(PipelineError::IngestError(msg)) = result {\n            assert!(msg.contains(\"parse error\"));\n        } else {\n            panic!(\"Expected IngestError\");\n        }\n    }\n\n    #[test]\n    fn test_transform_stage_hashing() {\n        let transform = TransformStage::new(\"urn:knhk:schema:test\".to_string(), false);\n        \n        let ingest_result = IngestResult {\n            triples: vec![\n                RawTriple {\n                    subject: \"http://example.org/subject\".to_string(),\n                    predicate: \"http://example.org/predicate\".to_string(),\n                    object: \"http://example.org/object\".to_string(),\n                    graph: None,\n                }\n            ],\n            metadata: BTreeMap::new(),\n        };\n        \n        let result = transform.transform(ingest_result);\n        assert!(result.is_ok());\n        \n        let transform_result = result.unwrap();\n        assert_eq!(transform_result.typed_triples.len(), 1);\n        assert!(transform_result.typed_triples[0].subject > 0);\n        assert!(transform_result.typed_triples[0].predicate > 0);\n        assert!(transform_result.typed_triples[0].object > 0);\n    }\n\n    #[test]\n    fn test_load_stage_predicate_grouping() {\n        let load = LoadStage::new();\n        \n        let transform_result = TransformResult {\n            typed_triples: vec![\n                TypedTriple { subject: 1, predicate: 100, object: 10, graph: None },\n                TypedTriple { subject: 2, predicate: 100, object: 20, graph: None },\n                TypedTriple { subject: 3, predicate: 200, object: 30, graph: None },\n            ],\n            validation_errors: Vec::new(),\n        };\n        \n        let result = load.load(transform_result);\n        assert!(result.is_ok());\n        \n        let load_result = result.unwrap();\n        assert_eq!(load_result.runs.len(), 2); // Two different predicates\n        assert_eq!(load_result.runs[0].pred, 100);\n        assert_eq!(load_result.runs[0].len, 2);\n        assert_eq!(load_result.runs[1].pred, 200);\n        assert_eq!(load_result.runs[1].len, 1);\n    }\n\n    #[test]\n    fn test_reflex_stage_tick_budget() {\n        let reflex = ReflexStage::new();\n        \n        let mut soa = SoAArrays::new();\n        soa.s[0] = 1;\n        soa.p[0] = 100;\n        soa.o[0] = 10;\n        \n        let run = PredRun { pred: 100, off: 0, len: 1 };\n        \n        let load_result = LoadResult {\n            soa_arrays: soa,\n            runs: vec![run],\n        };\n        \n        let result = reflex.reflex(load_result);\n        assert!(result.is_ok());\n        \n        let reflex_result = result.unwrap();\n        assert!(reflex_result.max_ticks <= 8);\n        assert!(!reflex_result.receipts.is_empty());\n    }\n\n    #[test]\n    fn test_receipt_merging() {\n        let receipt1 = Receipt {\n            id: \"r1\".to_string(),\n            ticks: 4,\n            lanes: 8,\n            span_id: 0x1234,\n            a_hash: 0xABCD,\n        };\n        \n        let receipt2 = Receipt {\n            id: \"r2\".to_string(),\n            ticks: 6,\n            lanes: 8,\n            span_id: 0x5678,\n            a_hash: 0xEF00,\n        };\n        \n        let merged = ReflexStage::merge_receipts(&[receipt1, receipt2]);\n        \n        assert_eq!(merged.ticks, 6); // Max ticks\n        assert_eq!(merged.lanes, 16); // Sum lanes\n        assert_eq!(merged.span_id, 0x1234 ^ 0x5678); // XOR merge\n        assert_eq!(merged.a_hash, 0xABCD ^ 0xEF00); // XOR merge\n    }\n\n    #[test]\n    fn test_emit_stage() {\n        let emit = EmitStage::new(true, vec![\"https://webhook.example.com\".to_string()]);\n        \n        let receipt = Receipt {\n            id: \"receipt1\".to_string(),\n            ticks: 4,\n            lanes: 8,\n            span_id: 0x1234,\n            a_hash: 0xABCD,\n        };\n        \n        let reflex_result = ReflexResult {\n            actions: vec![\n                Action {\n                    id: \"action1\".to_string(),\n                    payload: vec![1, 2, 3],\n                    receipt_id: \"receipt1\".to_string(),\n                }\n            ],\n            receipts: vec![receipt],\n            max_ticks: 4,\n        };\n        \n        let result = emit.emit(reflex_result);\n        assert!(result.is_ok());\n        \n        let emit_result = result.unwrap();\n        assert_eq!(emit_result.receipts_written, 1);\n        assert_eq!(emit_result.actions_sent, 1);\n        assert_eq!(emit_result.lockchain_hashes.len(), 1);\n    }\n}\n\n"
        }
    ]
}